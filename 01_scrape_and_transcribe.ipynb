{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yt-dlp openai-whisper pydub\n",
    "\n",
    "# Dependencies (systemwide)\n",
    "#sudo apt install ffmpeg  # Linux\n",
    "#brew install ffmpeg      # macOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b37af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import whisper\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab7a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_channel_meta(channel_url, limits):\n",
    "  \n",
    "  # use this kind of address as channel_url: https://www.youtube.com/@creator/videos\n",
    "  # limits is how many videos from the playlist (\"videos\" is also interpreted as playlist) you want to grab in descending order\n",
    "\n",
    "  options = {\n",
    "          'ignoreerrors': True,\n",
    "          'playlistend': limits,\n",
    "          'sleep_interval': 1,\n",
    "          'max_sleep_interval': 3,\n",
    "          'cookiesfrombrowser': ('chromium',),\n",
    "      }\n",
    "\n",
    "  with yt_dlp.YoutubeDL(options) as ydl:\n",
    "    info = ydl.extract_info(channel_url, download=False)\n",
    "    \n",
    "    if not info:\n",
    "        raise ValueError(\"Failed to retrieve channel metadata.\")\n",
    "  return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d6231b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_urls(channel_info):\n",
    "    return [item[\"webpage_url\"] for item in channel_info[\"entries\"] if item is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import time\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d6959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_video(video_url, start_count, data_dir=\"data\", media_dir = \"media\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Downloads audio from a YouTube video and returns a structured metadata dictionary.\n",
    "\n",
    "    Args:\n",
    "        video_url (str): Full URL to the YouTube video.\n",
    "        start_count (int): Index to label the entry (video) uniquely.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (result_dict, updated_start_count)\n",
    "    \"\"\"\n",
    "\n",
    "    options = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'sleep_interval': 2,\n",
    "        'max_sleep_interval': 5,\n",
    "        'cookiesfrombrowser': ('chromium',),\n",
    "    }\n",
    "\n",
    "    # Try to extract metadata first\n",
    "    with yt_dlp.YoutubeDL(options) as ydl:\n",
    "        try:\n",
    "            info = ydl.extract_info(video_url, download=False)\n",
    "            errors = None\n",
    "            if not isinstance(info, dict):\n",
    "                raise ValueError(\"No metadata returned.\")\n",
    "        except Exception as e:\n",
    "            errors = str(e)\n",
    "            info = {\n",
    "                \"title\": f\"no_title_{int(time.time())}\",\n",
    "                \"duration\": 0,\n",
    "                \"uploader\": \"unknown\",\n",
    "                \"view_count\": 0,\n",
    "                \"upload_date\": \"unknown\",\n",
    "            }\n",
    "\n",
    "    # Normalize file/folder names\n",
    "    max_file = 200\n",
    "    max_dir = 7\n",
    "\n",
    "    title = info[\"title\"].lower()\n",
    "    normalized_title = re.sub(r'[\\\\/*?:\"<>|!]', \"\", title)\n",
    "    normalized_title = re.sub(r\"[^\\x00-\\x7F]\", \"\", normalized_title)\n",
    "    filename_part = re.sub(r'\\s+', \"_\", normalized_title.strip())[:max_file]\n",
    "\n",
    "    uploader = info.get(\"uploader\", \"unknown\").lower()\n",
    "    directory = re.sub(r'[^\\w\\d]', \"\", uploader.strip())[:max_dir].ljust(max_dir, \"x\")\n",
    "\n",
    "    subfolder_path = os.path.join(data_dir, media_dir, directory)\n",
    "\n",
    "    if not os.path.exists(subfolder_path):\n",
    "        os.makedirs(subfolder_path)\n",
    "\n",
    "    output_file = os.path.join(subfolder_path, f\"{start_count}_{directory}_{filename_part}.%(ext)s\")\n",
    "    options['outtmpl'] = output_file\n",
    "\n",
    "    # Download actual audio\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(options) as ydl:\n",
    "            ydl.download([video_url])\n",
    "    except Exception as e:\n",
    "        errors = str(e)\n",
    "\n",
    "    # Final metadata dictionary\n",
    "    result = {\n",
    "        \"entry_no\": start_count,\n",
    "        \"title\": info[\"title\"],\n",
    "        \"directory\": directory,\n",
    "        \"filename\": f\"{start_count}_{directory}_{filename_part}.webm\",\n",
    "        \"duration\": info[\"duration\"],\n",
    "        \"uploader\": uploader,\n",
    "        \"view_count\": info[\"view_count\"],\n",
    "        \"upload_date\": info[\"upload_date\"],\n",
    "        \"url\": video_url,\n",
    "        \"errors\": errors,\n",
    "    }\n",
    "\n",
    "    return result, start_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52394419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_videos(video_urls, start_count):\n",
    "    \n",
    "    \"\"\"\n",
    "    Downloads a batch of YouTube videos and returns their metadata.\n",
    "\n",
    "    Args:\n",
    "        video_urls (list of str): List of full YouTube video URLs.\n",
    "        start_count (int): Starting index for naming and tracking.\n",
    "\n",
    "    Returns:\n",
    "        list: List of metadata dictionaries for each downloaded video.\n",
    "    \"\"\"\n",
    "    \n",
    "    downloaded_videos = []\n",
    "    \n",
    "    for url in video_urls:\n",
    "        result, start_count = process_single_video(url, start_count)\n",
    "        downloaded_videos.append(result)\n",
    "    \n",
    "    return downloaded_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_transcribe(downloaded_videos_db, data_dir=\"data\", media_dir = \"media\", model=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transcribes audio files from the downloaded metadata database.\n",
    "\n",
    "    Args:\n",
    "        downloaded_videos_db (list): List of metadata dictionaries from video downloads (by default received as the output of \"batch process videos\").\n",
    "        data_dir (str): Path to root data directory where audio files are stored.\n",
    "\n",
    "    Returns:\n",
    "        list: Updated list of metadata dictionaries, each with a \"transcript\" key.\n",
    "    \"\"\"\n",
    "\n",
    "    if model is None:\n",
    "        model = whisper.load_model(\"medium\")\n",
    "\n",
    "    transcript_dir = \"transcripts\"\n",
    "    transcript_path = os.path.join(data_dir, transcript_dir)\n",
    "\n",
    "    if not os.path.exists(transcript_path):\n",
    "            os.makedirs(transcript_path)\n",
    "\n",
    "    for item in downloaded_videos_db:\n",
    "        directory = item[\"directory\"]\n",
    "        filename = item[\"filename\"]\n",
    "        file_path = os.path.join(data_dir, media_dir, directory, filename)\n",
    "        uploader_transcript_dir = os.path.join(transcript_path, directory)\n",
    "        \n",
    "        if not os.path.exists(uploader_transcript_dir):\n",
    "            os.makedirs(uploader_transcript_dir)\n",
    "        \n",
    "        base_filename, _ = os.path.splitext(filename)\n",
    "        transcript_file = os.path.join(uploader_transcript_dir, base_filename)\n",
    "\n",
    "        if not os.path.isfile(file_path):\n",
    "            item[\"transcript\"] = \"File missing — could not transcribe.\"\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            result = model.transcribe(\n",
    "                file_path,\n",
    "                language=\"en\",\n",
    "                task=\"transcribe\",\n",
    "                temperature=0.0,\n",
    "                best_of=1,\n",
    "                fp16=False,\n",
    "                no_speech_threshold=0.6,\n",
    "                condition_on_previous_text=False\n",
    "            )\n",
    "            item[\"transcript\"] = result[\"text\"]\n",
    "            \n",
    "            with open(f\"{transcript_file}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(result[\"text\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            item[\"transcript\"] = f\"Transcription error: {str(e)}\"\n",
    "\n",
    "    return downloaded_videos_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec83414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_json(db, name_of_file):\n",
    "    \"\"\"\n",
    "    Exports a list of dictionaries to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        db (list): The data to export.\n",
    "        name_of_file (str): File path or name (without .json extension).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(db, list):\n",
    "            json_file = f\"{name_of_file}.json\"\n",
    "            with open(json_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(db, file, indent=4, ensure_ascii=False)\n",
    "            print(f\"Exported to {json_file}\")\n",
    "        else:\n",
    "            print(f\"Invalid DB type: {type(db)}. Expected a list.\")\n",
    "    except Exception as e:\n",
    "        print(f\"JSON export failed for {name_of_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = \"\"\"\n",
    "https://www.youtube.com/@creator_1/videos\n",
    "https://www.youtube.com/@creator_2/videos\n",
    "\"\"\"\n",
    "\n",
    "channel_urls = channels.strip().split()\n",
    "print(f\"Total channels: {len(channel_urls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bff3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_db = []\n",
    "start_count = 0\n",
    "model = whisper.load_model(\"medium\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "for url in channel_urls:\n",
    "    try:\n",
    "        channel_info = scrape_channel_meta(url, 2)\n",
    "        video_urls = get_video_urls(channel_info)\n",
    "\n",
    "        print(f\"\\n{url} — Found {len(video_urls)} videos\")\n",
    "\n",
    "        downloaded = []\n",
    "        for video_url in video_urls:\n",
    "            result, start_count = process_single_video(video_url, start_count)\n",
    "            downloaded.append(result)\n",
    "\n",
    "        transcribed = batch_transcribe(downloaded, model=model)\n",
    "        full_db.extend(transcribed)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {str(e)}\")\n",
    "\n",
    "databases_dir = \"corpus\"\n",
    "data_dir = \"data\"\n",
    "output_dir = os.path.join(data_dir, databases_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "export_to_json(full_db, os.path.join(output_dir, \"channels_full_db\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
